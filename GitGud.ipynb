{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p9G7fLaol4g"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai python-dotenv requests gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "NEBIUS_KEY = userdata.get(\"NEBIUS_API_KEY\")\n",
        "OPENAI_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "NEBIUS_BASE = \"https://api.studio.nebius.com/v1\"\n",
        "\n",
        "GIT_API_KEY = userdata.get(\"GIT_API_KEY\")\n",
        "HEADERS = {\"Authorization\": f\"token {GIT_API_KEY}\"}"
      ],
      "metadata": {
        "id": "C-94clTCqLnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    base_url=NEBIUS_BASE,\n",
        "    api_key=NEBIUS_KEY,\n",
        "    temperature=0.6,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "print(\"LLM Ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbIb1Ffwo_kx",
        "outputId": "425affd1-cadc-4b1e-db73-3080a3cbb6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-m0ULhn3tLW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=NEBIUS_KEY, base_url=NEBIUS_BASE)\n",
        "\n",
        "def classify_domain(user_input):\n",
        "    prompt = f\"\"\"\n",
        "    Identify the technical domain of the following user query.\n",
        "\n",
        "    Possible domains:\n",
        "    - web development\n",
        "    - backend engineering\n",
        "    - frontend engineering\n",
        "    - networking\n",
        "    - data science\n",
        "    - machine learning\n",
        "    - artificial intelligence\n",
        "    - algorithms\n",
        "    - operating systems\n",
        "    - cybersecurity\n",
        "    - devops\n",
        "    - mobile development\n",
        "    - databases\n",
        "    - cloud computing\n",
        "    - robotics\n",
        "    - electronics\n",
        "    - embedded systems\n",
        "    - computer vision\n",
        "    - software architecture\n",
        "    - APIs and protocols\n",
        "\n",
        "    User Input: \"{user_input}\"\n",
        "\n",
        "    Respond with ONE domain only.\n",
        "    \"\"\"\n",
        "\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return res.choices[0].message.content.strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOozMQDn-VO"
      },
      "outputs": [],
      "source": [
        "def technical_query(user_input, domain):\n",
        "    prompt = f\"\"\"\n",
        "    Convert the user input into a concise technical query (3â€“5 words)\n",
        "    using domain-specific terminology.\n",
        "\n",
        "    Domain: {domain}\n",
        "\n",
        "    RULES:\n",
        "    - Output must be a technical noun phrase.\n",
        "    - Do NOT produce a sentence.\n",
        "    - Do NOT format the ouput in any way. Return plain text.\n",
        "    - Use domain-specific words like:\n",
        "        architecture, pipeline, connectivity,\n",
        "        interfacing, synchronization, protocols, binding,\n",
        "        backend, frontend, rendering, compute, framework.\n",
        "\n",
        "    User Input: \"{user_input}\"\n",
        "\n",
        "    Technical Query:\n",
        "    \"\"\"\n",
        "\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return res.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54cN47SFoCnG"
      },
      "outputs": [],
      "source": [
        "def make_query(user_input):\n",
        "    domain = classify_domain(user_input)\n",
        "    query = technical_query(user_input, domain)\n",
        "    return query"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "forks = 0\n",
        "sort = \"stars\"\n",
        "max_results = 5\n",
        "\n",
        "def scraped_data(query):\n",
        "  url = f\"https://api.github.com/search/repositories?q={query}+forks:>{forks}&sort=stars&order=desc&per_page={max_results}&page=1\"\n",
        "\n",
        "  res = requests.get(url, headers=HEADERS)\n",
        "  data = res.json()\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "ezNnje_4NQq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_repo_urls(query):\n",
        "\n",
        "  repo_urls = []\n",
        "  data = scraped_data(query)\n",
        "\n",
        "  for item in data.get(\"items\"):\n",
        "    repo_urls.append(item[\"html_url\"])\n",
        "  return repo_urls\n",
        "\n",
        "  return repo_urls"
      ],
      "metadata": {
        "id": "yzgK1Z0nrKOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLSEOR2uI8-w"
      },
      "outputs": [],
      "source": [
        "def fetch_repo_data(repo_url):\n",
        "    \"\"\"\n",
        "    Fetch repository information from GitHub API\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parts = repo_url.strip().rstrip('/').split('/')\n",
        "        owner = parts[-2]\n",
        "        repo_name = parts[-1]\n",
        "        api_url = f\"https://api.github.com/repos/{owner}/{repo_name}\"\n",
        "        response = requests.get(api_url, headers=HEADERS)\n",
        "        if response.status_code != 200:\n",
        "            return None, f\"Error: Unable to fetch repository data (Status {response.status_code})\"\n",
        "        repo_data = response.json()\n",
        "        readme_url = f\"https://api.github.com/repos/{owner}/{repo_name}/readme\"\n",
        "        readme_response = requests.get(readme_url, headers=HEADERS)\n",
        "        readme_content = \"\"\n",
        "        if readme_response.status_code == 200:\n",
        "            import base64\n",
        "            readme_data = readme_response.json()\n",
        "            readme_content = base64.b64decode(readme_data['content']).decode('utf-8')[:2000]\n",
        "        compiled_data = {\n",
        "            \"name\": repo_data.get(\"name\", \"N/A\"),\n",
        "            \"description\": repo_data.get(\"description\", \"N/A\"),\n",
        "            \"language\": repo_data.get(\"language\", \"N/A\"),\n",
        "            \"stars\": repo_data.get(\"stargazers_count\", 0),\n",
        "            \"forks\": repo_data.get(\"forks_count\", 0),\n",
        "            \"topics\": repo_data.get(\"topics\", []),\n",
        "            \"readme_snippet\": readme_content,\n",
        "        }\n",
        "        return compiled_data, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Error parsing repository: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_repo_data(repo_urls):\n",
        "\n",
        "  all_repos_data = [] #list of dicts\n",
        "\n",
        "  for repo_url in repo_urls:\n",
        "    repo_data = fetch_repo_data(repo_url)\n",
        "    all_repos_data.append(repo_data[0])\n",
        "\n",
        "  return all_repos_data"
      ],
      "metadata": {
        "id": "MwNxYR6FsWQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16d93e6f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "import json\n",
        "\n",
        "def filter_and_summarize_repos(llm, user_prompt, repo_data_list):\n",
        "    \"\"\"Feed top 5 repos to LLM B and get comparative summary + best repo suggestion.\"\"\"\n",
        "    context = \"\"\n",
        "    for repo in repo_data_list:\n",
        "        context += f\"Repo:\\nName: {repo['name']}\\nDescription: {repo.get('description', '')}\\nStars: {repo.get('stars', 0)}\\nLanguage: {repo.get('language', '')}\\nREADME (truncated):\\n{repo.get('readme', '')[:1000]}\\n\\n\"\n",
        "\n",
        "    template = PromptTemplate(\n",
        "        input_variables=[\"user_prompt\", \"context\"],\n",
        "        template=\"\"\"You are an expert assistant comparing GitHub repositories for a user project.\n",
        "\n",
        "User's request:\n",
        "{user_prompt}\n",
        "\n",
        "Below are summaries of a few repositories. Analyze and answer:\n",
        "\n",
        "1. Which repository is MOST relevant to the user's goal (dont refer to it by number)?\n",
        "2. Why? Give a short justification.\n",
        "3. Provide a short summary of that repo:\n",
        "  a. Provide a clear, concise summary of the repository\n",
        "  b. Identify the main purpose and technology stack\n",
        "  c. Highlight notable features or patterns\n",
        "  d. Give instructions on how to use the repository\n",
        "Be specific, helpful, and constructive.\n",
        "\n",
        "Repositories data:\n",
        "{context}\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "    prompt_text = template.format(user_prompt=user_prompt, context=context)\n",
        "    response = llm.invoke(prompt_text)\n",
        "\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c5f6ba3"
      },
      "outputs": [],
      "source": [
        "def gradio_wrapper(user_input):\n",
        "  repo_urls = fetch_repo_urls(query)\n",
        "  all_repos_data = list_repo_data(repo_urls)\n",
        "  output = filter_and_summarize_repos(llm, user_input, all_repos_data)\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "<h1 style='text-align:center;'>\n",
        "GitGud\n",
        "</h1>\"\"\")\n",
        "    link = gr.Textbox(label=\"What can I help you with?\")\n",
        "    btn = gr.Button(\"Analyze\")\n",
        "    output = gr.Markdown()\n",
        "    btn.click(gradio_wrapper, inputs=link, outputs=output)\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "ZkxXwP_hItod",
        "outputId": "360ba5a4-739b-4de4-be5e-0c3eec227ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c7427195cc44f6f963.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c7427195cc44f6f963.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkoiWIU7X1EP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}